{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU",
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "064dc20b488b44b48661c8433988bc6e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2a2cf06d66344dc3b83625e6d9cb76d1",
       "IPY_MODEL_a58655b230a04025a875d4066e141d00",
       "IPY_MODEL_d983212181984c3493d35f2d5e3ef095"
      ],
      "layout": "IPY_MODEL_9bf346292f314899b44d704e63c45bf4"
     }
    },
    "2a2cf06d66344dc3b83625e6d9cb76d1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_db13e3748f964c43b7acf6e7327b1e3a",
      "placeholder": "​",
      "style": "IPY_MODEL_4b5c611e7a144c2fb6b821ff86889599",
      "value": "model.safetensors: 100%"
     }
    },
    "a58655b230a04025a875d4066e141d00": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_84e0c7aff30d43288b0320fd151d4e89",
      "max": 49335454,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6d3b9ca807f14ea69030e85d12af5e42",
      "value": 49335454
     }
    },
    "d983212181984c3493d35f2d5e3ef095": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_15bdd5d9183c48d984343b335219ffdb",
      "placeholder": "​",
      "style": "IPY_MODEL_dbb9073558a045439f15a7c5770aaf2c",
      "value": " 49.3M/49.3M [00:00&lt;00:00, 59.3MB/s]"
     }
    },
    "9bf346292f314899b44d704e63c45bf4": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "db13e3748f964c43b7acf6e7327b1e3a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4b5c611e7a144c2fb6b821ff86889599": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "84e0c7aff30d43288b0320fd151d4e89": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6d3b9ca807f14ea69030e85d12af5e42": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "15bdd5d9183c48d984343b335219ffdb": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dbb9073558a045439f15a7c5770aaf2c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "!pip install git+https://github.com/jacobgil/pytorch-grad-cam.git -q"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6Hjnr-ibO2AB",
    "outputId": "f7bd7c2f-f004-4443-d59b-33d102386bc3"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import shutil\n",
    "import kagglehub\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix,ConfusionMatrixDisplay\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.utils.data import DataLoader, random_split,WeightedRandomSampler , Dataset\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import timm\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "metadata": {
    "id": "vtuFWRr7hlbE"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8QsvGZE4gS-k",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "2c18b57d-edb5-4b3e-fb57-a831d17ada75"
   },
   "source": [
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"salmansajid05/oral-diseases\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "83b35a11"
   },
   "source": [
    "### Subtask:\n",
    "Load a pre-trained YOLO model and configure it for the number of classes in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b1290d38"
   },
   "source": [
    "**Reasoning**:\n",
    "Import the `YOLO` class from the `ultralytics` library and load a pre-trained model, such as `yolov8n.pt`. Then, configure the model for training by specifying the path to the data configuration file (`oral_diseases.yaml`) and the number of training epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "05c80394"
   },
   "source": [
    "### Subtask:\n",
    "Create a YAML file that specifies the paths to the training and validation data and the class names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ade2022"
   },
   "source": [
    "**Reasoning**:\n",
    "Create a YAML file named `oral_diseases.yaml` in the `/kaggle/working/yolo_dataset` directory. This file will contain the paths to the training and validation image directories, the number of classes, and a list of class names."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#Define original directories\n",
    "original_dirs = {'Calculus': '/kaggle/input/oral-diseases/Calculus/Calculus',\n",
    "               'Caries' : '/kaggle/input/oral-diseases/Data caries/Data caries/caries augmented data set/preview',\n",
    "               'Gingivitis' : '/kaggle/input/oral-diseases/Gingivitis/Gingivitis',\n",
    "               'Mouth Ulcer' : '/kaggle/input/oral-diseases/Mouth Ulcer/Mouth Ulcer/Mouth_Ulcer_augmented_DataSet/preview',\n",
    "               'Tooth Discoloration' : '/kaggle/input/oral-diseases/Tooth Discoloration/Tooth Discoloration /Tooth_discoloration_augmented_dataser/preview',\n",
    "               'hypodontia' : '/kaggle/input/oral-diseases/hypodontia/hypodontia'}\n",
    "\n",
    "#Define target base directory\n",
    "base_dir = \"/kaggle/working/dataset\"\n",
    "splits = ['train','val','test']\n",
    "classes = list(original_dirs.keys())\n",
    "\n",
    "#Create target directories\n",
    "for split in splits:\n",
    "    for class_name in classes :\n",
    "        os.makedirs(os.path.join(base_dir,split,class_name),exist_ok = True)\n",
    "\n",
    "#Function to copy and split images\n",
    "def copy_and_transfer_images(class_name ,image_paths):\n",
    "    train_path , test_path = train_test_split(image_paths , test_size = 0.1, random_state =42 )\n",
    "    train_path , val_path = train_test_split(train_path , test_size = 0.15, random_state =42 )\n",
    "    split_path = {'train': train_path , 'val' : val_path , 'test' : test_path}\n",
    "\n",
    "    for split, paths in split_path.items():\n",
    "        for img_path in paths :\n",
    "            target_path = os.path.join(base_dir , split , class_name , os.path.basename(img_path))\n",
    "            shutil.copy(img_path, target_path)\n",
    "\n",
    "#Organize dataset\n",
    "for class_name , original_dir in original_dirs.items():\n",
    "    image_paths = [os.path.join(root,file) for root, _ , files in os.walk(original_dir)\n",
    "                   for file in files if file.endswith(('.jpeg','.jpg','.png'))]\n",
    "    if image_paths:\n",
    "        copy_and_transfer_images(class_name ,image_paths)\n",
    "\n",
    "print(\"Images organized successfully.\")"
   ],
   "metadata": {
    "id": "9LCNj8regblc",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "bafb7d09-8994-48f5-c6d3-738cdd04fd9c"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Number of images to display per class and per row\n",
    "num_images_per_class = 4\n",
    "num_images_per_row = 4\n",
    "\n",
    "# Function to fetch random images from a given directory\n",
    "def get_random_images(directory , num_images):\n",
    "    \"\"\"\n",
    "    Returns a list of random image paths from the given directory.\n",
    "    Supports jpg, jpeg, and png formats.\n",
    "    \"\"\"\n",
    "    all_images = [os.path.join(directory, image) for image in os.listdir(directory)\n",
    "                  if image.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    if len(all_images) <= num_images:\n",
    "        return all_images\n",
    "\n",
    "    return random.sample(all_images,num_images)\n",
    "\n",
    "# Calculate the number of rows needed for displaying images\n",
    "total_images = len(classes)*num_images_per_class\n",
    "num_rows = (total_images + num_images_per_row - 1)// num_images_per_row\n",
    "\n",
    "# Create the figure and subplots\n",
    "fig, axes = plt.subplots(num_rows, num_images_per_row ,figsize = (15,num_rows*2.5) )\n",
    "fig.suptitle(\"Random Images from Each Class\", fontsize=16)\n",
    "\n",
    "# Flatten the axes array for easy indexing\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Keep track of the subplot index\n",
    "image_index = 0\n",
    "\n",
    "# Loop through each class and display random images\n",
    "for class_name in classes:\n",
    "    class_path = os.path.join(base_dir,'train',class_name)\n",
    "    random_images = get_random_images(class_path,num_images_per_class)\n",
    "\n",
    "    for img_path in random_images:\n",
    "        if image_index <len(axes):\n",
    "            image = Image.open(img_path)\n",
    "            axes[image_index].imshow(image)\n",
    "            axes[image_index].set_title(class_name)\n",
    "            axes[image_index].axis('off')\n",
    "            image_index += 1\n",
    "\n",
    "\n",
    "# Hide any remaining unused subplots\n",
    "for i in range(image_index, len(axes)):\n",
    "    axes[i].axis('off')\n",
    "\n",
    "# Adjust spacing and show the plot\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "DmJDXRkzhAeT",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 619
    },
    "outputId": "30f44bd2-7b82-41fd-eda9-9268ba90824c"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "#  Function to count the number of images in each split (train, val, test) for a given class\n",
    "def count_images(base_dir, class_name):\n",
    "    counts = {}\n",
    "    for split in [\"train\", \"val\", \"test\"]:\n",
    "        split_dir = os.path.join(base_dir, split, class_name)\n",
    "        counts[split] = len(os.listdir(split_dir))\n",
    "    return counts\n",
    "\n",
    "# Count images for each class and store results in a dictionary\n",
    "class_split_counts = {class_name: count_images(base_dir, class_name) for class_name in classes}\n",
    "\n",
    "# Create subplots: 3 rows × 2 columns (total 6 plots)\n",
    "fig, axes = plt.subplots(3, 2, figsize=(12, 12))\n",
    "axes = axes.flatten()  # Flatten the 2D array of axes into a 1D array for easy access\n",
    "\n",
    "# Loop through the first 6 classes and create bar plots\n",
    "for idx, class_name in enumerate(classes[:6]):\n",
    "    counts = class_split_counts[class_name]\n",
    "    ax = axes[idx]\n",
    "\n",
    "    # Create a bar chart for this class (Train, Val, Test)\n",
    "    ax.bar(counts.keys(), counts.values(), color= ['green','yellow','red'])\n",
    "\n",
    "    # Add titles and labels\n",
    "    ax.set_title(f\"{class_name}\", fontsize=12)\n",
    "    ax.set_xlabel(\"Dataset Split\")\n",
    "    ax.set_ylabel(\"Number of Images\")\n",
    "    ax.set_ylim(0, max(counts.values()) + 10)  # Set y-axis limit slightly above max for spacing\n",
    "\n",
    "    # Display the count values above the bars\n",
    "    for i, val in enumerate(counts.values()):\n",
    "        ax.text(i, val + 1, str(val), ha=\"center\", fontsize=10)\n",
    "\n",
    "# Hide any unused subplots if there are fewer than 6 classes\n",
    "for j in range(len(classes[:6]), len(axes)):\n",
    "    axes[j].axis(\"off\")\n",
    "\n",
    "# Adjust layout for better visualization\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "jBLmORbMhDLH",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 633
    },
    "outputId": "deb5b33e-f303-42ec-93e1-e18c7d325c3e"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Create a pie chart showing the distribution of classes in the training set\n",
    "train_counts = {class_name: counts[\"train\"] for class_name, counts in class_split_counts.items()}\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.pie(train_counts.values(), labels=train_counts.keys(), autopct='%1.1f%%', startangle=140)\n",
    "plt.title(\"Distribution of Classes in Training Set\")\n",
    "plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "q8AAzwZghT3X",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 583
    },
    "outputId": "10a9242c-37a7-44be-c758-0d5bbec67148"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert the dictionary to a Pandas DataFrame for easier plotting\n",
    "df_counts = pd.DataFrame(class_split_counts).T\n",
    "df_counts = df_counts.sort_index()\n",
    "\n",
    "# Create a grouped bar chart\n",
    "ax = df_counts.plot(kind='bar', figsize=(12, 7), rot=0)\n",
    "\n",
    "# Add titles and labels\n",
    "plt.title('Distribution of Images per Class Across Splits', fontsize=16)\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Number of Images')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.legend(title='Split')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Add counts on top of bars\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container, fmt='%d', label_type='edge', padding=3)\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "LZsTvsJGz2Ht",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "outputId": "aa2bf76c-8250-43d5-87a0-d6cb54f98de6"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ad65bad8",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 621
    },
    "outputId": "35b1a11b-5977-498c-e4eb-342bae505155"
   },
   "source": [
    "# Function to display random images from each class\n",
    "def display_random_images_from_classes(base_dir, classes, num_images_per_class=4):\n",
    "    \"\"\"\n",
    "    Displays a grid of random images from each specified class in the training set.\n",
    "    \"\"\"\n",
    "    num_classes = len(classes)\n",
    "    num_rows = (num_classes * num_images_per_class + num_images_per_row - 1) // num_images_per_row\n",
    "\n",
    "    fig, axes = plt.subplots(num_rows, num_images_per_row, figsize=(15, num_rows * 2.5))\n",
    "    fig.suptitle(\"Random Images from Each Class (Training Set)\", fontsize=16)\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    img_index = 0\n",
    "    for class_name in classes:\n",
    "        class_path = os.path.join(base_dir, 'train', class_name)\n",
    "        random_images = get_random_images(class_path, num_images_per_class)\n",
    "\n",
    "        for img_path in random_images:\n",
    "            if img_index < len(axes):\n",
    "                image = Image.open(img_path)\n",
    "                axes[img_index].imshow(image)\n",
    "                axes[img_index].set_title(class_name)\n",
    "                axes[img_index].axis('off')\n",
    "                img_index += 1\n",
    "\n",
    "    for i in range(img_index, len(axes)):\n",
    "        axes[i].axis('off')\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "# Display random images from the training set\n",
    "display_random_images_from_classes(base_dir, classes)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Create a pie chart showing the distribution of classes in the training set\n",
    "train_counts = {class_name: counts[\"train\"] for class_name, counts in class_split_counts.items()}\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.pie(train_counts.values(), labels=train_counts.keys(), autopct='%1.1f%%', startangle=140)\n",
    "plt.title(\"Distribution of Classes in Training Set\")\n",
    "plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "DhUjnd5QjP_0",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 583
    },
    "outputId": "1499a65a-3e42-4aa2-a6eb-383c8c546c08"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "f75398ac"
   },
   "source": [
    "#  Function to count the number of images in each split (train, val, test) for a given class\n",
    "def count_images(base_dir, class_name):\n",
    "    counts = {}\n",
    "    for split in [\"train\", \"val\", \"test\"]:\n",
    "        split_dir = os.path.join(base_dir, split, class_name)\n",
    "        counts[split] = len(os.listdir(split_dir))\n",
    "    return counts\n",
    "\n",
    "# Count images for each class and store results in a dictionary\n",
    "class_split_counts = {class_name: count_images(base_dir, class_name) for class_name in classes}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "d99db5ff",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 583
    },
    "outputId": "f2a09fa1-e7fa-4fdf-c030-701dc58f4bd9"
   },
   "source": [
    "# Create a pie chart showing the distribution of classes in the training set\n",
    "train_counts = {class_name: counts[\"train\"] for class_name, counts in class_split_counts.items()}\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.pie(train_counts.values(), labels=train_counts.keys(), autopct='%1.1f%%', startangle=140)\n",
    "plt.title(\"Distribution of Classes in Training Set\")\n",
    "plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming class_split_counts is already defined from previous steps\n",
    "# Convert the dictionary to a Pandas DataFrame for easier display as a table\n",
    "df_counts = pd.DataFrame(class_split_counts).T\n",
    "\n",
    "# Display the DataFrame as a table\n",
    "display(df_counts)"
   ],
   "metadata": {
    "id": "Chz23G7Tkc7Z",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "outputId": "ff34a810-b54d-4d57-fa53-c7d539fb0c26"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "a558e841",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 169
    },
    "outputId": "41eb25ae-f354-4c22-9d6e-6e8cbb1f7071"
   },
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 3)) # Adjust figure size as needed\n",
    "ax.axis('off') # Hide axes\n",
    "ax.axis('tight') # Adjust layout\n",
    "\n",
    "# Create the table\n",
    "table = ax.table(cellText=df_counts.values,\n",
    "                 colLabels=df_counts.columns,\n",
    "                 rowLabels=df_counts.index,\n",
    "                 loc='center')\n",
    "\n",
    "# Style the table (optional)\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1.2, 1.2) # Adjust scale as needed\n",
    "\n",
    "plt.title(\"Distribution of Images per Class Across Splits (Table)\", fontsize=14)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model Creation"
   ],
   "metadata": {
    "id": "TcR42WnqlveH"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "train_transforms = A.Compose([\n",
    "    # Randomly crop and resize the image to focus on important regions like teeth and gums\n",
    "    A.RandomResizedCrop(\n",
    "        size =(300,300), scale=(0.95, 1.0), p=1.0\n",
    "    ),\n",
    "\n",
    "    # Randomly flip the image horizontally or vertically to simulate different viewing angles\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.1),\n",
    "\n",
    "    # Slight shift, scaling, and rotation to simulate different head positions\n",
    "    A.ShiftScaleRotate(\n",
    "        shift_limit=0.05,      # up to ±5% image shift\n",
    "        scale_limit=0.05,      # up to ±5% zoom in/out\n",
    "        rotate_limit=15,        # max ±7 degrees rotation\n",
    "        border_mode=0,         # fill empty areas with black pixels\n",
    "        p=0.4\n",
    "    ),\n",
    "\n",
    "    # Adjust brightness and contrast to make infections, tissues, and gums clearer\n",
    "    A.RandomBrightnessContrast(\n",
    "        brightness_limit=0.15,  # ±20% brightness adjustment\n",
    "        contrast_limit=0.15,   # ±25% contrast adjustment\n",
    "        p=0.4\n",
    "    ),\n",
    "\n",
    "    # Apply CLAHE to locally enhance image contrast and highlight teeth/gum problems\n",
    "    A.CLAHE(\n",
    "        clip_limit=2.5,\n",
    "        tile_grid_size=(8, 8),\n",
    "        p=0.4\n",
    "    ),\n",
    "\n",
    "    # Slight color variations to simulate different lighting conditions in images\n",
    "    A.HueSaturationValue(\n",
    "        hue_shift_limit=8,     # small hue shift\n",
    "        sat_shift_limit=20,    # adjust saturation\n",
    "        val_shift_limit=10,    # adjust brightness\n",
    "        p=0.4\n",
    "    ),\n",
    "\n",
    "    # Normalize image based on ImageNet statistics → suitable for InceptionV3 / EfficientNet-B3\n",
    "    A.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    ),\n",
    "\n",
    "    # Convert image to PyTorch tensor format (C, H, W)\n",
    "    ToTensorV2(),\n",
    "], p=1.0)\n",
    "\n",
    "# ===========================\n",
    "# VALIDATION TRANSFORMATIONS\n",
    "# ===========================\n",
    "val_transforms = A.Compose([\n",
    "    # Resize image without random cropping to keep validation images consistent\n",
    "    A.Resize(height=300, width=300),\n",
    "\n",
    "    # Normalize using the same ImageNet statistics as training\n",
    "    A.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    ),\n",
    "\n",
    "    # Convert image to PyTorch tensor format\n",
    "    ToTensorV2(),\n",
    "], p=1.0)"
   ],
   "metadata": {
    "id": "PgGK6tcSnDh5"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Custom Dataset to integrate Albumentations\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, classes,transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.classes = classes\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
    "        label = self.labels[idx]\n",
    "        img = np.array(img)\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(image=img)[\"image\"]\n",
    "\n",
    "        return img, label"
   ],
   "metadata": {
    "id": "Ok8jNoQsluse"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Define directories\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "val_dir = os.path.join(base_dir, 'val')\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "\n",
    "# Use ImageFolder to read image paths and labels automatically\n",
    "train_data = ImageFolder(root=train_dir)\n",
    "val_data = ImageFolder(root=val_dir)\n",
    "test_data = ImageFolder(root=test_dir)\n",
    "\n",
    "classes = train_data.classes\n",
    "\n",
    "# Extract image paths and labels from ImageFolder\n",
    "train_image_paths = [path for path, _ in train_data.samples]\n",
    "train_labels = [label for _, label in train_data.samples]\n",
    "\n",
    "val_image_paths = [path for path, _ in val_data.samples]\n",
    "val_labels = [label for _, label in val_data.samples]\n",
    "\n",
    "test_image_paths = [path for path, _ in test_data.samples]\n",
    "test_labels = [label for _, label in test_data.samples]\n",
    "\n",
    "# Create datasets using CustomImageDataset + Albumentations\n",
    "train_dataset = CustomImageDataset(train_image_paths, train_labels,classes, transform=train_transforms)\n",
    "val_dataset = CustomImageDataset(val_image_paths, val_labels,classes, transform=val_transforms)\n",
    "test_dataset = CustomImageDataset(test_image_paths, test_labels,classes, transform=val_transforms)\n",
    "\n",
    "# Class counts (your data)\n",
    "class_counts = np.array([991, 1821, 990, 1943, 1402, 260])\n",
    "\n",
    "# Compute weights for each class\n",
    "class_weights = 1. / torch.tensor(class_counts, dtype=torch.float)\n",
    "\n",
    "# Map weights to each sample in the training dataset\n",
    "train_labels_tensor = torch.tensor(train_labels, dtype=torch.long)\n",
    "sample_weights = class_weights[train_labels_tensor]\n",
    "\n",
    "# WeightedRandomSampler to handle class imbalance\n",
    "sampler = WeightedRandomSampler(\n",
    "    weights=sample_weights,\n",
    "    num_samples=len(sample_weights),\n",
    "    replacement=True\n",
    ")\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32 , sampler=sampler, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4, pin_memory=True)"
   ],
   "metadata": {
    "id": "0SvV7yu8lzMX"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def show_MRI_batch(dataloader,title = 'Batch of Images'):\n",
    "    images ,labels = next(iter(dataloader))\n",
    "    fig , axes = plt.subplots(4,8,figsize = (15,10))\n",
    "    fig.suptitle(title)\n",
    "    for i,ax in enumerate(axes.flatten()):\n",
    "        if i <len(images):\n",
    "            img = images[i].permute(1,2,0)\n",
    "            ax.imshow(img)\n",
    "            ax.set_title(train_dataset.classes[labels[i]])\n",
    "            ax.axis('off')\n",
    "\n",
    "    plt.show()"
   ],
   "metadata": {
    "id": "pIMPv68CmWWh"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "show_MRI_batch(train_loader)"
   ],
   "metadata": {
    "id": "G8OcE5Esnd33",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "outputId": "2cb8178b-0c94-42cb-bfd9-99562c5d185c"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training and fitting a model"
   ],
   "metadata": {
    "id": "1PcHG0MloYXd"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class CustomEfficientnet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CustomEfficientnet,self).__init__()\n",
    "\n",
    "        # Load pretrained ResNet50\n",
    "        self.model = timm.create_model('efficientnet_b3',pretrained = True)\n",
    "\n",
    "        # unFreeze layers\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        # Replace the fully connected layer with a custom classifier\n",
    "        in_features = self.model.classifier.in_features\n",
    "\n",
    "        self.model.classifier = nn.Sequential(\n",
    "            nn.Linear(in_features,1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(1024,512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(512,256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256,128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128,num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.model(x)"
   ],
   "metadata": {
    "id": "Bxwd0EkCnhFQ"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Number of images per class (your data)\n",
    "class_counts = torch.tensor([991, 1821, 990, 1943, 1402, 260], dtype=torch.float)\n",
    "\n",
    "# Calculate class weights inversely proportional to class frequency\n",
    "# Formula: weight = total_samples / (num_classes * class_count)\n",
    "class_weights = 0\n",
    "class_weights = class_counts.sum() / (len(class_counts) * class_counts)\n",
    "\n",
    "# Move to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class_weights = class_weights.to(device)\n",
    "\n",
    "print(\"Class names:\", classes,'\\n')\n",
    "print(\"Class Weights:\", class_weights,'\\n')"
   ],
   "metadata": {
    "id": "4zj002BQoblL",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "1882eebf-2686-42a8-ad1b-622cf2098fb3"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "num_classes = len(train_dataset.classes)\n",
    "model = CustomEfficientnet(num_classes)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr = 0.001, weight_decay = 0.00001)\n",
    "criterion = nn.CrossEntropyLoss(class_weights)\n",
    "scheduler = ReduceLROnPlateau(optimizer , mode = 'min' , factor = 0.2 , patience = 2)"
   ],
   "metadata": {
    "id": "4DsJgqj5oyLf",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "064dc20b488b44b48661c8433988bc6e",
      "2a2cf06d66344dc3b83625e6d9cb76d1",
      "a58655b230a04025a875d4066e141d00",
      "d983212181984c3493d35f2d5e3ef095",
      "9bf346292f314899b44d704e63c45bf4",
      "db13e3748f964c43b7acf6e7327b1e3a",
      "4b5c611e7a144c2fb6b821ff86889599",
      "84e0c7aff30d43288b0320fd151d4e89",
      "6d3b9ca807f14ea69030e85d12af5e42",
      "15bdd5d9183c48d984343b335219ffdb",
      "dbb9073558a045439f15a7c5770aaf2c"
     ]
    },
    "outputId": "16314181-f3d8-4bb8-c30b-9471b9ef940e"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# num_epochs = 25\n",
    "\n",
    "# scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "# best_model_wts = None\n",
    "# best_val_loss = float(\"inf\")\n",
    "\n",
    "# train_loss_list = []\n",
    "# train_acc_list = []\n",
    "# val_loss_list = []\n",
    "# val_acc_list = []\n",
    "\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "\n",
    "#     model.train()\n",
    "#     running_loss = 0.0\n",
    "#     train_correct = 0\n",
    "#     train_total = 0\n",
    "\n",
    "#     for images, labels in tqdm(train_loader):\n",
    "#         images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         with torch.cuda.amp.autocast():\n",
    "#             outputs = model(images)\n",
    "#             loss = criterion(outputs, labels)\n",
    "\n",
    "#         scaler.scale(loss).backward()\n",
    "#         scaler.step(optimizer)\n",
    "#         scaler.update()\n",
    "\n",
    "#         running_loss += loss.item()\n",
    "\n",
    "#         _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "#         train_total += labels.size(0)\n",
    "#         train_correct += (predicted == labels).sum().item()\n",
    "\n",
    "#     avg_train_loss = running_loss / len(train_loader)\n",
    "#     train_accuracy = 100 * train_correct / train_total\n",
    "\n",
    "#     train_loss_list.append(avg_train_loss)\n",
    "#     train_acc_list.append(train_accuracy)\n",
    "\n",
    "\n",
    "#     model.eval()\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     val_loss = 0.0\n",
    "\n",
    "#     all_labels = []\n",
    "#     all_predictions = []\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for images, labels in tqdm(val_loader):\n",
    "#             images, labels = images.to(device), labels.to(device)\n",
    "#             outputs = model(images)\n",
    "\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             val_loss += loss.item()\n",
    "\n",
    "#             _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "#             all_labels.extend(labels.cpu().numpy())\n",
    "#             all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "#             total += labels.size(0)\n",
    "#             correct += (predicted == labels).sum().item()\n",
    "\n",
    "#     avg_val_loss = val_loss / len(val_loader)\n",
    "#     accuracy = 100 * correct / total\n",
    "\n",
    "#     val_loss_list.append(avg_val_loss)\n",
    "#     val_acc_list.append(accuracy)\n",
    "\n",
    "#     scheduler.step(avg_val_loss)\n",
    "\n",
    "#     if avg_val_loss < best_val_loss:\n",
    "#         best_val_loss = avg_val_loss\n",
    "#         best_model_wts = model.state_dict().copy()\n",
    "#         torch.save(best_model_wts, \"best_model.pth\")\n",
    "#         print(\"✅ Model Saved! Best so far.\")\n",
    "\n",
    "#     print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
    "#       f\"Train Loss: {avg_train_loss:.4f} | \"\n",
    "#       f\"Val Loss: {avg_val_loss:.4f} | \"\n",
    "#       f\"Train Accuracy: {train_accuracy:.2f}% | \"\n",
    "#       f\"Val Accuracy: {accuracy:.2f}% | \"\n",
    "#       f\"LR: {optimizer.param_groups[0]['lr']:.8f} |\")"
   ],
   "metadata": {
    "id": "J505Cg-Co1Pw"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# epochs = range(1, len(train_loss_list) + 1)\n",
    "\n",
    "# plt.figure(figsize=(12,5))\n",
    "\n",
    "# # Loss Plot\n",
    "# plt.subplot(1,2,1)\n",
    "# plt.plot(epochs, train_loss_list, 'b-o', label='Train Loss')\n",
    "# plt.plot(epochs, val_loss_list , 'r-o', label='Validation Loss')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.title('Train vs Validation Loss')\n",
    "# plt.legend()\n",
    "\n",
    "# # Accuracy Plot\n",
    "# plt.subplot(1,2,2)\n",
    "# plt.plot(epochs, train_acc_list, 'g-o', label='Train Accuracy')\n",
    "# plt.plot(epochs, val_acc_list, 'm-o', label='Validation Accuracy')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.title('Train vs Validation Accuracy')\n",
    "# plt.legend()\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ],
   "metadata": {
    "id": "hggG0Qu-qmHG",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 251
    },
    "outputId": "b82de751-a316-4561-b300-c264e48d397d"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Calculate confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_predictions)\n",
    "\n",
    "# Display confusion matrix with larger figure\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "# disp.plot(ax=ax, cmap=\"Reds\")\n",
    "plt.title(\"Confusion Matrix\", fontsize=16)\n",
    "plt.xticks(rotation=45, fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "S6AGPJqcFVaI"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import classification_report\n",
    "#\n",
    "print(classification_report(all_labels, all_predictions, target_names=classes))"
   ],
   "metadata": {
    "id": "TCk2ZLGLFbu1"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model evaluation"
   ],
   "metadata": {
    "id": "zY-OxkfpFeJk"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "show_MRI_batch(test_loader)"
   ],
   "metadata": {
    "id": "7iD-AuIlFkKV"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "best_model =  CustomEfficientnet(num_classes)\n",
    "best_model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "best_model.to(device)\n",
    "print('best_model ready')"
   ],
   "metadata": {
    "id": "gIbwxadlFmjE"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "best_model.eval()\n",
    "\n",
    "test_labels = []\n",
    "test_predictions = []\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "\n",
    "        outputs = best_model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        test_labels.extend(labels.cpu().numpy())\n",
    "        test_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Accuracy on Test Set: {accuracy:.2f}%\")"
   ],
   "metadata": {
    "id": "cR0Q9C8XFp65"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Calculate confusion matrix\n",
    "cm = confusion_matrix(test_labels, test_predictions)\n",
    "\n",
    "# Display confusion matrix with larger figure\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "disp.plot(ax=ax, cmap=\"Reds\")\n",
    "plt.title(\"Confusion Matrix\", fontsize=16)\n",
    "plt.xticks(rotation=45, fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "Ep6XV8JWFs3o"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "print(classification_report(test_labels,test_predictions, target_names=classes))"
   ],
   "metadata": {
    "id": "3JmecfM0Fvx5"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def visualize_grad_cam(image_paths, model, device, class_names):\n",
    "    num_images = len(image_paths)\n",
    "    num_rows = (num_images + 1) // 2\n",
    "    plt.figure(figsize=(20, 5 * num_rows))\n",
    "\n",
    "    # EfficientNet من timm: آخر بلوك conv\n",
    "    target_layer = model.model.blocks[-1]\n",
    "    grad_cam = GradCAM(model=model, target_layers=[target_layer])\n",
    "\n",
    "    for idx, image_path in enumerate(image_paths):\n",
    "        input_image = Image.open(image_path).convert('RGB')\n",
    "        preprocess = transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "        input_tensor = preprocess(input_image).unsqueeze(0).to(device)\n",
    "\n",
    "        grayscale_cam = grad_cam(input_tensor)[0]\n",
    "        input_image_np = np.array(input_image.resize((224, 224))) / 255.0\n",
    "        visualization = show_cam_on_image(input_image_np, grayscale_cam, use_rgb=True)\n",
    "\n",
    "        outputs = model(input_tensor)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        predicted_class = class_names[predicted.item()]\n",
    "        true_class = os.path.basename(os.path.dirname(image_path))\n",
    "        title_color = 'green' if true_class == predicted_class else 'red'\n",
    "\n",
    "        plt.subplot(num_rows, 4, 2 * idx + 1)\n",
    "        plt.imshow(input_image_np)\n",
    "        plt.title(f'True: {true_class}', fontsize=24, color=title_color)\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(num_rows, 4, 2 * idx + 2)\n",
    "        plt.imshow(visualization)\n",
    "        plt.title(f'Predicted: {predicted_class}', fontsize=24, color=title_color)\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ],
   "metadata": {
    "id": "T6VQohUbF2c2"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Get random test images and visualize Grad-CAM\n",
    "random_images = [os.path.join(test_dir, cls, random.choice(os.listdir(os.path.join(test_dir, cls))))\n",
    "                 for cls in classes for _ in range(2)]\n",
    "visualize_grad_cam(random_images, model, device, classes)"
   ],
   "metadata": {
    "id": "uvgOx6-kF48j"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "df3990f7"
   },
   "source": [
    "# Evaluate the best Custom EfficientNet model on the test set\n",
    "\n",
    "best_model.eval()\n",
    "\n",
    "test_labels_clf = []\n",
    "test_predictions_clf = []\n",
    "\n",
    "correct_clf = 0\n",
    "total_clf = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        outputs = best_model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        test_labels_clf.extend(labels.cpu().numpy())\n",
    "        test_predictions_clf.extend(predicted.cpu().numpy())\n",
    "\n",
    "        correct_clf += (predicted == labels).sum().item()\n",
    "        total_clf += labels.size(0)\n",
    "\n",
    "accuracy_clf = 100 * correct_clf / total_clf\n",
    "print(f\"Accuracy of Custom EfficientNet on Test Set: {accuracy_clf:.2f}%\")\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nClassification Report for Custom EfficientNet:\")\n",
    "print(classification_report(test_labels_clf, test_predictions_clf, target_names=classes))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm_clf = confusion_matrix(test_labels_clf, test_predictions_clf)\n",
    "disp_clf = ConfusionMatrixDisplay(confusion_matrix=cm_clf, display_labels=classes)\n",
    "fig_clf, ax_clf = plt.subplots(figsize=(8, 8))\n",
    "disp_clf.plot(ax=ax_clf, cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix for Custom EfficientNet\", fontsize=16)\n",
    "plt.xticks(rotation=45, fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fine tuning a yolo model\n"
   ],
   "metadata": {
    "id": "VkmggtL_GsZa"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "20cc8085"
   },
   "source": [
    "# Task\n",
    "Fine-tune a YOLO model using the images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "14ccfad8"
   },
   "source": [
    "## Install ultralytics\n",
    "\n",
    "### Subtask:\n",
    "Install the necessary library for YOLO models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fead2d1e"
   },
   "source": [
    "**Reasoning**:\n",
    "Install the ultralytics library using pip.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "d3266fdc",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "5f53704a-6e07-48fa-d3fc-dc973e38da38"
   },
   "source": [
    "!pip install ultralytics"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a42dac44"
   },
   "source": [
    "## Prepare data for yolo\n",
    "\n",
    "### Subtask:\n",
    "Convert the existing image dataset into the format required by YOLO, which includes creating text files with bounding box annotations for each image.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d7870a10"
   },
   "source": [
    "**Reasoning**:\n",
    "Create the directory structure for the YOLO dataset and then iterate through the existing dataset to copy images and create dummy label files with a single bounding box around the entire image.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import shutil\n",
    "from PIL import Image\n",
    "\n",
    "# Define the base directory for the YOLO dataset\n",
    "yolo_base_dir = \"/kaggle/working/yolo_dataset\"\n",
    "\n",
    "# Define the structure for YOLO data\n",
    "yolo_data_dirs = {\n",
    "    'train': os.path.join(yolo_base_dir, 'images', 'train'),\n",
    "    'val': os.path.join(yolo_base_dir, 'images', 'val'),\n",
    "    'test': os.path.join(yolo_base_dir, 'images', 'test'),\n",
    "    'train_labels': os.path.join(yolo_base_dir, 'labels', 'train'),\n",
    "    'val_labels': os.path.join(yolo_base_dir, 'labels', 'val'),\n",
    "    'test_labels': os.path.join(yolo_base_dir, 'labels', 'test')\n",
    "}\n",
    "\n",
    "# Create the directories\n",
    "for dir_path in yolo_data_dirs.values():\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "# Function to create a dummy YOLO label file for an image\n",
    "def create_dummy_label(image_path, label_path, class_id):\n",
    "    try:\n",
    "        img = Image.open(image_path)\n",
    "        width, height = img.size\n",
    "        # Create a dummy bounding box covering the whole image\n",
    "        # Format: class_id center_x center_y width height (normalized)\n",
    "        dummy_bbox = f\"{class_id} 0.5 0.5 1.0 1.0\"\n",
    "        with open(label_path, 'w') as f:\n",
    "            f.write(dummy_bbox)\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating dummy label for {image_path}: {e}\")\n",
    "\n",
    "\n",
    "# Copy images and create dummy labels\n",
    "for split in ['train', 'val', 'test']:\n",
    "    split_image_dir = yolo_data_dirs[split]\n",
    "    split_label_dir = yolo_data_dirs[f'{split}_labels']\n",
    "\n",
    "    # Use the original_dirs mapping to get the original paths\n",
    "    for class_name, original_dir in original_dirs.items():\n",
    "        # Find the class_id for the current class_name\n",
    "        class_id = classes.index(class_name)\n",
    "\n",
    "        # Get image paths from the original directory\n",
    "        image_paths = [os.path.join(root,file) for root, _ , files in os.walk(original_dir)\n",
    "                   for file in files if file.endswith(('.jpeg','.jpg','.png'))]\n",
    "\n",
    "        # Split image paths for the current split (assuming copy_and_transfer_images did the split already)\n",
    "        # If not, a splitting logic would be needed here based on the original paths\n",
    "        # For simplicity, I'll assume the files are already in the 'base_dir' structure\n",
    "        # that was created in the previous steps and use that as the source.\n",
    "        source_dir = os.path.join(base_dir, split, class_name)\n",
    "        if os.path.exists(source_dir):\n",
    "            for image_name in os.listdir(source_dir):\n",
    "                if image_name.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                    original_image_path = os.path.join(source_dir, image_name)\n",
    "                    target_image_path = os.path.join(split_image_dir, image_name)\n",
    "                    label_name = os.path.splitext(image_name)[0] + '.txt'\n",
    "                    target_label_path = os.path.join(split_label_dir, label_name)\n",
    "\n",
    "                    # Copy image\n",
    "                    shutil.copy(original_image_path, target_image_path)\n",
    "\n",
    "                    # Create dummy label\n",
    "                    create_dummy_label(original_image_path, target_label_path, class_id)\n",
    "\n",
    "\n",
    "print(\"YOLO dataset structure created and dummy labels generated.\")"
   ],
   "metadata": {
    "id": "B7tj7EREPYzG",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "fdbfa173-c4e8-4e5d-afbd-142836827312"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Create the data.yaml file\n",
    "data_yaml_content = f\"\"\"\n",
    "path: {yolo_base_dir}\n",
    "train: images/train\n",
    "val: images/val\n",
    "test: images/test\n",
    "\n",
    "nc: {len(classes)}\n",
    "names: {classes}\n",
    "\"\"\"\n",
    "\n",
    "with open(os.path.join(yolo_base_dir, 'oral_diseases.yaml'), 'w') as f:\n",
    "    f.write(data_yaml_content)\n",
    "\n",
    "print(\"YOLO configuration file 'oral_diseases.yaml' created.\")"
   ],
   "metadata": {
    "id": "MoEW_nE5-QDU",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "96c931ac-a044-4354-c6bb-c496940e3119"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a pre-trained YOLO model (e.g., yolov8n.pt)\n",
    "model = YOLO('yolov8n.pt')\n",
    "\n",
    "# Configure the model for training\n",
    "# The data argument specifies the path to your dataset configuration file\n",
    "# The epochs argument sets the number of training epochs\n",
    "results = model.train(data=os.path.join(yolo_base_dir, 'oral_diseases.yaml'), epochs=25)"
   ],
   "metadata": {
    "id": "_emCIt0w-VyY",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "10804fac-ca14-4101-f023-f4e61a0834ce"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ee246c04"
   },
   "source": [
    "## Model Testing and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "41c24a47"
   },
   "source": [
    "### Evaluate Custom EfficientNet Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "28a662fd"
   },
   "source": [
    "### Evaluate YOLO Model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "c8e6d550",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "outputId": "c8cb3f64-796a-433b-bc07-c8631a751c25"
   },
   "source": [
    "# Run inference on a few test images using the fine-tuned YOLO model\n",
    "# and visualize the predictions with bounding boxes.\n",
    "\n",
    "# Get a few random image paths from the test set\n",
    "random_test_image_paths = []\n",
    "num_images_to_show = 8 # You can adjust this number\n",
    "for cls in classes:\n",
    "    class_path = os.path.join(test_dir, cls)\n",
    "    if os.path.exists(class_path):\n",
    "        images_in_class = [os.path.join(class_path, img) for img in os.listdir(class_path) if img.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        if images_in_class:\n",
    "            random_test_image_paths.extend(random.sample(images_in_class, min(2, len(images_in_class)))) # Get up to 2 images per class\n",
    "\n",
    "# Load the best trained YOLO model\n",
    "# The weights are saved in runs/detect/train/weights/best.pt (or trainX if you ran multiple trainings)\n",
    "yolo_model_path = '/content/runs/detect/train/weights/best.pt' # Update path if necessary\n",
    "yolo_model = YOLO(yolo_model_path)\n",
    "\n",
    "print(\"\\nRunning YOLO inference on sample test images:\")\n",
    "yolo_results = yolo_model(random_test_image_paths)\n",
    "\n",
    "# Visualize the results\n",
    "for result in yolo_results:\n",
    "    # The result object contains the image with predictions drawn on it\n",
    "    display(Image.fromarray(result.plot()[:,:,::-1]))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0ebc96a9",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 358
    },
    "outputId": "a161b5a8-3c5f-4c20-deba-d2a9194a17c7"
   },
   "source": [
    "# Evaluate the YOLO model on the test set\n",
    "print(\"\\nEvaluating YOLO model on the test set:\")\n",
    "yolo_test_results = yolo_model.val()\n",
    "\n",
    "print(\"\\nYOLO Model Evaluation Metrics on Test Set:\")\n",
    "# The metrics are stored in the 'box' attribute of the results object\n",
    "display(yolo_test_results.box.map)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "pRmTaHbkChZR"
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}
